#!/usr/bin/env python3
"""
í•˜ì´ë¸Œë¦¬ë“œ Whisper ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸
ë¡œì»¬ + OpenAI API ëª¨ë“œ ëª¨ë‘ í…ŒìŠ¤íŠ¸
"""
import os
import sys
import asyncio
from pathlib import Path

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from faster_whisper import WhisperModel
from auto_subtitle.openai_client_simple import openai_whisper_client

async def test_hybrid_system(audio_file_path: str):
    """í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ ì „ì²´ í…ŒìŠ¤íŠ¸"""
    
    print("ğŸ§ª í•˜ì´ë¸Œë¦¬ë“œ Whisper ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("="*60)
    
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not os.path.exists(audio_file_path):
        print(f"âŒ ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {audio_file_path}")
        return False
    
    file_size = os.path.getsize(audio_file_path)
    print(f"ğŸ“ íŒŒì¼ ì •ë³´: {os.path.basename(audio_file_path)} ({file_size/1024:.1f}KB)")
    print()
    
    # 1ë‹¨ê³„: ë¡œì»¬ ëª¨ë“œ í…ŒìŠ¤íŠ¸
    print("ğŸ  1ë‹¨ê³„: ë¡œì»¬ Faster-Whisper í…ŒìŠ¤íŠ¸")
    print("-" * 40)
    
    local_success = await test_local_mode(audio_file_path)
    print()
    
    # 2ë‹¨ê³„: OpenAI API ëª¨ë“œ í…ŒìŠ¤íŠ¸  
    print("ğŸŒ 2ë‹¨ê³„: OpenAI API ëª¨ë“œ í…ŒìŠ¤íŠ¸")
    print("-" * 40)
    
    api_success = await test_api_mode(audio_file_path)
    print()
    
    # 3ë‹¨ê³„: í•˜ì´ë¸Œë¦¬ë“œ ë¡œì§ í…ŒìŠ¤íŠ¸
    print("ğŸ”„ 3ë‹¨ê³„: í•˜ì´ë¸Œë¦¬ë“œ ë¡œì§ í…ŒìŠ¤íŠ¸")
    print("-" * 40)
    
    hybrid_success = await test_hybrid_logic(audio_file_path)
    print()
    
    # ê²°ê³¼ ìš”ì•½
    print("ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    print("=" * 60)
    print(f"ğŸ  ë¡œì»¬ ëª¨ë“œ:     {'âœ… ì„±ê³µ' if local_success else 'âŒ ì‹¤íŒ¨'}")
    print(f"ğŸŒ OpenAI API:   {'âœ… ì„±ê³µ' if api_success else 'âŒ ì‹¤íŒ¨ (API í‚¤ í•„ìš”)'}")
    print(f"ğŸ”„ í•˜ì´ë¸Œë¦¬ë“œ:    {'âœ… ì„±ê³µ' if hybrid_success else 'âŒ ì‹¤íŒ¨'}")
    
    return local_success or api_success

async def test_local_mode(audio_path: str) -> bool:
    """ë¡œì»¬ ëª¨ë“œ í…ŒìŠ¤íŠ¸"""
    try:
        print("ğŸ“¥ Faster-Whisper ëª¨ë¸ ë¡œë“œ ì¤‘...")
        model = WhisperModel("large-v3", device="cpu", compute_type="int8")
        print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        
        print("ğŸ¯ ë¡œì»¬ ìŒì„± ì¸ì‹ ì¤‘...")
        segments, info = model.transcribe(
            audio_path, 
            language="ko",
            task="transcribe"
        )
        
        segments_list = []
        full_text = ""
        for segment in segments:
            text = segment.text.strip()
            if text:
                segments_list.append(text)
                full_text += text + " "
        
        print(f"âœ… ë¡œì»¬ ì „ì‚¬ ì™„ë£Œ!")
        print(f"   ì–¸ì–´: {info.language} (í™•ë¥ : {info.language_probability:.2f})")
        print(f"   í…ìŠ¤íŠ¸: {full_text.strip()}")
        print(f"   ì„¸ê·¸ë¨¼íŠ¸: {len(segments_list)}ê°œ")
        
        return True
        
    except Exception as e:
        print(f"âŒ ë¡œì»¬ ëª¨ë“œ ì‹¤íŒ¨: {e}")
        return False

async def test_api_mode(audio_path: str) -> bool:
    """OpenAI API ëª¨ë“œ í…ŒìŠ¤íŠ¸"""
    try:
        if not openai_whisper_client.is_available():
            print("âš ï¸ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
            print("ğŸ’¡ .env íŒŒì¼ì— OPENAI_API_KEY ì„¤ì • í•„ìš”")
            return False
        
        print("ğŸŒ OpenAI API ì „ì‚¬ ì¤‘...")
        result = await openai_whisper_client.transcribe_audio_api(audio_path, "ko")
        
        if result.get("success"):
            print(f"âœ… API ì „ì‚¬ ì™„ë£Œ!")
            print(f"   ì–¸ì–´: {result.get('language', 'unknown')}")
            print(f"   í…ìŠ¤íŠ¸: {result.get('text', '')}")
            print(f"   ì„¸ê·¸ë¨¼íŠ¸: {len(result.get('segments', []))}ê°œ")
            print(f"   íŒŒì¼ í¬ê¸°: {result.get('file_size_mb', 0):.1f}MB")
            return True
        else:
            print(f"âŒ API ì „ì‚¬ ì‹¤íŒ¨: {result.get('error', 'Unknown error')}")
            return False
            
    except Exception as e:
        print(f"âŒ API ëª¨ë“œ ì‹¤íŒ¨: {e}")
        return False

async def test_hybrid_logic(audio_path: str) -> bool:
    """í•˜ì´ë¸Œë¦¬ë“œ ë¡œì§ í…ŒìŠ¤íŠ¸ (API ìš°ì„ , ì‹¤íŒ¨ì‹œ ë¡œì»¬ ëŒ€ì²´)"""
    try:
        print("ğŸ”„ í•˜ì´ë¸Œë¦¬ë“œ ë¡œì§ ì‹¤í–‰...")
        
        # API ëª¨ë“œ ìš°ì„  ì‹œë„
        if openai_whisper_client.is_available():
            print("   â†’ API ëª¨ë“œ ì‹œë„...")
            api_result = await openai_whisper_client.transcribe_audio_api(audio_path, "ko")
            
            if api_result.get("success"):
                print("   âœ… API ëª¨ë“œ ì„±ê³µ!")
                print(f"   í…ìŠ¤íŠ¸: {api_result.get('text', '')}")
                return True
            else:
                print(f"   âš ï¸ API ëª¨ë“œ ì‹¤íŒ¨: {api_result.get('error')}")
                print("   â†’ ë¡œì»¬ ëª¨ë“œë¡œ ìë™ ëŒ€ì²´...")
        else:
            print("   â†’ API ì‚¬ìš© ë¶ˆê°€, ë¡œì»¬ ëª¨ë“œë¡œ ì§„í–‰...")
        
        # ë¡œì»¬ ëª¨ë“œë¡œ ëŒ€ì²´
        print("   â†’ ë¡œì»¬ ëª¨ë“œ ì‹¤í–‰...")
        model = WhisperModel("large-v3", device="cpu", compute_type="int8")
        segments, info = model.transcribe(audio_path, language="ko")
        
        full_text = ""
        for segment in segments:
            text = segment.text.strip()
            if text:
                full_text += text + " "
        
        print("   âœ… ë¡œì»¬ ëª¨ë“œ ì„±ê³µ!")
        print(f"   í…ìŠ¤íŠ¸: {full_text.strip()}")
        return True
        
    except Exception as e:
        print(f"âŒ í•˜ì´ë¸Œë¦¬ë“œ ë¡œì§ ì‹¤íŒ¨: {e}")
        return False

if __name__ == "__main__":
    if len(sys.argv) != 2:
        print("ì‚¬ìš©ë²•: python test_hybrid_system.py <audio_file_path>")
        sys.exit(1)
    
    audio_path = sys.argv[1]
    success = asyncio.run(test_hybrid_system(audio_path))
    
    if success:
        print("\nğŸ‰ ì „ì²´ í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
    else:
        print("\nğŸ’¥ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨!")
