"""
OpenAI Whisper API í´ë¼ì´ì–¸íŠ¸ ëª¨ë“ˆ
LexGlu/whisper-audio-transcriber ë°©ì‹ì„ ì°¸ì¡°í•˜ì—¬ êµ¬í˜„
"""
import asyncio
import os
import tempfile
from typing import List, Dict, Optional, Tuple
from datetime import datetime
import aiofiles
from dotenv import load_dotenv
from openai import OpenAI
from pydub import AudioSegment
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# 10ë¶„ ì²­í¬ ì„¤ì • (25MB ì œí•œ ì¤€ìˆ˜)
CHUNK_DURATION_MS = 10 * 60 * 1000

class OpenAIWhisperClient:
    """OpenAI Whisper API í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self):
        self.client: Optional[OpenAI] = None
        self.api_key: Optional[str] = None
        self.max_audio_length: int = 10  # ê¸°ë³¸ 10ë¶„
        self._load_environment()
    
    def _load_environment(self):
        """í™˜ê²½ë³€ìˆ˜ ë¡œë“œ"""
        try:
            load_dotenv()
            self.api_key = os.getenv("OPENAI_API_KEY")
            self.max_audio_length = int(os.getenv("MAX_AUDIO_LENGTH_MINUTES", "10"))
            
            if self.api_key and self.api_key != "your_openai_api_key_here":
                self.client = OpenAI(api_key=self.api_key)
                logger.info("âœ… OpenAI API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
            else:
                logger.warning("âš ï¸ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ - ë¡œì»¬ ëª¨ë“œë§Œ ì‚¬ìš© ê°€ëŠ¥")
        except Exception as e:
            logger.error(f"âŒ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def is_available(self) -> bool:
        """API ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        return self.client is not None
    
    async def read_audio_file(self, audio_path: str) -> AudioSegment:
        """ë¹„ë™ê¸°ë¡œ ì˜¤ë””ì˜¤ íŒŒì¼ ì½ê¸°"""
        return await asyncio.to_thread(AudioSegment.from_file, audio_path)
    
    def split_audio_chunks(self, audio: AudioSegment) -> List[AudioSegment]:
        """ì˜¤ë””ì˜¤ë¥¼ ì²­í¬ë¡œ ë¶„í•  (10ë¶„ ë‹¨ìœ„)"""
        chunks = []
        total_duration = len(audio)
        
        for start in range(0, total_duration, CHUNK_DURATION_MS):
            end = min(start + CHUNK_DURATION_MS, total_duration)
            chunk = audio[start:end]
            chunks.append(chunk)
        
        logger.info(f"ğŸ“Š ì˜¤ë””ì˜¤ ë¶„í•  ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬ ({total_duration/1000/60:.1f}ë¶„)")
        return chunks
    
    async def export_chunk_temp(self, chunk: AudioSegment, index: int) -> str:
        """ì²­í¬ë¥¼ ì„ì‹œ íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
        temp_file = tempfile.NamedTemporaryFile(
            suffix=f"_chunk_{index}.mp3", 
            delete=False
        )
        temp_path = temp_file.name
        temp_file.close()
        
        # ë¹„ë™ê¸°ë¡œ ì²­í¬ ë‚´ë³´ë‚´ê¸°
        await asyncio.to_thread(chunk.export, temp_path, format="mp3")
        return temp_path
    
    async def transcribe_chunk_api(
        self, 
        chunk_path: str, 
        language: str = "ko"
    ) -> Dict:
        """OpenAI APIë¡œ ì²­í¬ ì „ì‚¬"""
        def _api_call():
            with open(chunk_path, "rb") as audio_file:
                transcript = self.client.audio.transcriptions.create(
                    model="whisper-1",
                    file=audio_file,
                    language=language,
                    response_format="verbose_json",
                    timestamp_granularities=["segment"]
                )
                return transcript
        
        try:
            result = await asyncio.to_thread(_api_call)
            return {
                "text": result.text,
                "segments": [
                    {
                        "start": seg.start,
                        "end": seg.end,
                        "text": seg.text.strip()
                    }
                    for seg in result.segments
                ],
                "language": getattr(result, 'language', language)
            }
        except Exception as e:
            logger.error(f"âŒ API ì „ì‚¬ ì‹¤íŒ¨: {e}")
            raise e
    
    async def process_chunk(
        self, 
        chunk: AudioSegment, 
        index: int, 
        language: str = "ko"
    ) -> Dict:
        """ì²­í¬ ì²˜ë¦¬ (ë‚´ë³´ë‚´ê¸° + ì „ì‚¬)"""
        chunk_path = None
        try:
            # ì²­í¬ë¥¼ ì„ì‹œ íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸°
            chunk_path = await self.export_chunk_temp(chunk, index)
            logger.info(f"ğŸµ ì²­í¬ {index} ì²˜ë¦¬ ì¤‘... ({chunk.duration_seconds:.1f}ì´ˆ)")
            
            # APIë¡œ ì „ì‚¬
            result = await self.transcribe_chunk_api(chunk_path, language)
            
            return {
                "success": True,
                "chunk_index": index,
                "duration": chunk.duration_seconds,
                **result
            }
        
        except Exception as e:
            logger.error(f"âŒ ì²­í¬ {index} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "chunk_index": index,
                "error": str(e)
            }
        
        finally:
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            if chunk_path and os.path.exists(chunk_path):
                try:
                    os.unlink(chunk_path)
                except:
                    pass
    
    def format_timestamp(self, seconds: float) -> str:
        """ì´ˆë¥¼ íƒ€ì„ìŠ¤íƒ¬í”„ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        return f"{hours:02d}:{minutes:02d}:{secs:02d}"
    
    def merge_segments(self, chunk_results: List[Dict]) -> List[Dict]:
        """ì²­í¬ë³„ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ì‹œê°„ ìˆœì„œë¡œ ë³‘í•©"""
        all_segments = []
        cumulative_time = 0.0
        
        for chunk_result in chunk_results:
            if not chunk_result.get("success"):
                continue
                
            chunk_duration = chunk_result.get("duration", 0)
            segments = chunk_result.get("segments", [])
            
            # ì„¸ê·¸ë¨¼íŠ¸ ì‹œê°„ ì¡°ì •
            for segment in segments:
                adjusted_segment = {
                    "start": segment["start"] + cumulative_time,
                    "end": segment["end"] + cumulative_time,
                    "text": segment["text"]
                }
                all_segments.append(adjusted_segment)
            
            cumulative_time += chunk_duration
        
        return all_segments
    
    async def transcribe_audio_api(
        self, 
        audio_path: str, 
        language: str = "ko"
    ) -> Dict:
        """ì „ì²´ API ì „ì‚¬ í”„ë¡œì„¸ìŠ¤ (ì²­í¬ ì²˜ë¦¬)"""
        try:
            if not self.is_available():
                raise Exception("OpenAI API í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
            
            logger.info(f"ğŸ¯ OpenAI API ì „ì‚¬ ì‹œì‘: {audio_path}")
            
            # ì˜¤ë””ì˜¤ íŒŒì¼ ì½ê¸°
            audio = await self.read_audio_file(audio_path)
            duration_minutes = audio.duration_seconds / 60
            
            # ê¸¸ì´ ì œí•œ í™•ì¸
            if duration_minutes > self.max_audio_length:
                raise Exception(f"ì˜¤ë””ì˜¤ ê¸¸ì´ ì œí•œ ì´ˆê³¼: {duration_minutes:.1f}ë¶„ > {self.max_audio_length}ë¶„")
            
            # ì²­í¬ ë¶„í• 
            chunks = self.split_audio_chunks(audio)
            
            # ë³‘ë ¬ ì²­í¬ ì²˜ë¦¬
            logger.info(f"âš¡ {len(chunks)}ê°œ ì²­í¬ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘...")
            tasks = [
                self.process_chunk(chunk, i+1, language)
                for i, chunk in enumerate(chunks)
            ]
            chunk_results = await asyncio.gather(*tasks)
            
            # ì„±ê³µí•œ ì²­í¬ë§Œ í•„í„°ë§
            successful_chunks = [r for r in chunk_results if r.get("success")]
            if not successful_chunks:
                raise Exception("ëª¨ë“  ì²­í¬ ì²˜ë¦¬ ì‹¤íŒ¨")
            
            # ê²°ê³¼ ë³‘í•©
            all_text = " ".join([r["text"] for r in successful_chunks])
            all_segments = self.merge_segments(successful_chunks)
            
            logger.info(f"âœ… API ì „ì‚¬ ì™„ë£Œ: {len(all_segments)}ê°œ ì„¸ê·¸ë¨¼íŠ¸")
            
            return {
                "success": True,
                "text": all_text.strip(),
                "segments": all_segments,
                "language": language,
                "processing_method": "openai_api",
                "chunks_processed": len(successful_chunks),
                "total_duration": audio.duration_seconds
            }
        
        except Exception as e:
            logger.error(f"âŒ API ì „ì‚¬ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "processing_method": "openai_api"
            }

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
openai_whisper_client = OpenAIWhisperClient()
