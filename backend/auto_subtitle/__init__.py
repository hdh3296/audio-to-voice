"""
Auto-subtitle ëª¨ë“ˆ (í•˜ì´ë¸Œë¦¬ë“œ ë²„ì „)
faster-whisper + OpenAI API í†µí•©
"""
from faster_whisper import WhisperModel
import ffmpeg
import os
import tempfile
from pathlib import Path
from typing import Optional, Dict, List
import subprocess
import json
import asyncio
from dotenv import load_dotenv

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì„í¬íŠ¸
from .openai_client import openai_whisper_client

# GPT í›„ì²˜ë¦¬ ëª¨ë“ˆ ì„í¬íŠ¸ (ì§€ì—° ì„í¬íŠ¸ë¡œ ì˜ì¡´ì„± ë¬¸ì œ í•´ê²°)
try:
    from .gpt_postprocessor import gpt_postprocessor
    GPT_POSTPROCESSOR_AVAILABLE = True
    print("âœ… GPT í›„ì²˜ë¦¬ ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
except Exception as e:
    print(f"âš ï¸ GPT í›„ì²˜ë¦¬ ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")
    gpt_postprocessor = None
    GPT_POSTPROCESSOR_AVAILABLE = False

class AutoSubtitle:
    def __init__(self):
        self.models = {}
        load_dotenv()  # í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
    
    def load_model(self, model_name: str = "large-v3"):
        """Faster-Whisper ëª¨ë¸ ë¡œë“œ (í•œêµ­ì–´ ìµœì í™”, ì•ˆì •ì„± ìš°ì„ )"""
        if model_name not in self.models:
            print(f"ğŸ“¥ Faster-Whisper ëª¨ë¸ ë¡œë“œ ì¤‘: {model_name}")
            try:
                # ì•ˆì •ì„±ì„ ìœ„í•´ CPU ì‚¬ìš©, í•„ìš”ì‹œ GPUëŠ” ì‚¬ìš©ìê°€ ìˆ˜ë™ ì„¤ì •
                self.models[model_name] = WhisperModel(
                    model_name, 
                    device="cpu", 
                    compute_type="int8",
                    download_root=None,  # ê¸°ë³¸ ìºì‹œ ë””ë ‰í† ë¦¬ ì‚¬ìš©
                    local_files_only=False  # ì˜¨ë¼ì¸ ë‹¤ìš´ë¡œë“œ í—ˆìš©
                )
                print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_name}")
            except Exception as e:
                print(f"âŒ {model_name} ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                # large-v3 ì‹¤íŒ¨ì‹œ mediumìœ¼ë¡œ ìë™ ëŒ€ì²´
                if model_name == "large-v3":
                    print("ğŸ”„ medium ëª¨ë¸ë¡œ ìë™ ëŒ€ì²´ ì¤‘...")
                    fallback_model = "medium"
                    self.models[model_name] = WhisperModel(
                        fallback_model, 
                        device="cpu", 
                        compute_type="int8"
                    )
                    print(f"âœ… ëŒ€ì²´ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {fallback_model}")
                else:
                    raise e
        return self.models[model_name]
    
    def transcribe_audio_local(
        self, 
        audio_path: str, 
        model_name: str = "large-v3",
        language: Optional[str] = "ko",
        task: str = "transcribe"
    ) -> Dict:
        """ë¡œì»¬ Faster-Whisperë¡œ ì˜¤ë””ì˜¤ ì „ì‚¬"""
        try:
            model = self.load_model(model_name)
            
            # í•œêµ­ì–´ ìµœì í™” í”„ë¡¬í”„íŠ¸
            korean_prompt = "ì•ˆë…•í•˜ì„¸ìš”. ë‹¤ìŒì€ í•œêµ­ì–´ ìŒì„±ì…ë‹ˆë‹¤. ì •í™•í•œ ë¬¸ì¥ ë¶€í˜¸ì™€ ìì—°ìŠ¤ëŸ¬ìš´ ë„ì–´ì“°ê¸°ë¥¼ í¬í•¨í•´ ì£¼ì„¸ìš”."
            
            print(f"ğŸ¯ ë¡œì»¬ í•œêµ­ì–´ ìŒì„± ì¸ì‹ ì‹œì‘: {audio_path}")
            print(f"ğŸ“Š ëª¨ë¸: {model_name}, ì–¸ì–´: {language}")
            
            segments, info = model.transcribe(
                audio_path, 
                language=language,
                task=task,
                word_timestamps=True,
                initial_prompt=korean_prompt,
                # í•œêµ­ì–´ ìµœì í™” ì„¤ì •
                beam_size=5,
                best_of=5,
                temperature=0.0,
                condition_on_previous_text=True,
                compression_ratio_threshold=2.4,
                log_prob_threshold=-1.0,
                no_speech_threshold=0.6
            )
            
            # ê²°ê³¼ ìˆ˜ì§‘
            segments_list = []
            full_text = ""
            
            for segment in segments:
                cleaned_text = segment.text.strip()
                if cleaned_text:
                    segment_dict = {
                        "start": segment.start,
                        "end": segment.end,
                        "text": cleaned_text
                    }
                    segments_list.append(segment_dict)
                    full_text += cleaned_text + " "
            
            result = {
                "success": True,
                "text": full_text.strip(),
                "segments": segments_list,
                "language": info.language,
                "language_probability": info.language_probability,
                "processing_method": "local_faster_whisper"
            }
            
            print(f"âœ… ë¡œì»¬ ìŒì„± ì¸ì‹ ì™„ë£Œ: {len(segments_list)}ê°œ êµ¬ê°„")
            return result
        
        except Exception as e:
            print(f"âŒ ë¡œì»¬ ìŒì„± ì¸ì‹ ì‹¤íŒ¨: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "processing_method": "local_faster_whisper"
            }
    
    async def transcribe_audio_hybrid(
        self, 
        audio_path: str, 
        model_name: str = "large-v3",
        language: Optional[str] = "ko",
        task: str = "transcribe",
        use_api: bool = False,
        use_gpt_correction: bool = False
    ) -> Dict:
        """í•˜ì´ë¸Œë¦¬ë“œ ì „ì‚¬ (API ìš°ì„ , ì‹¤íŒ¨ì‹œ ë¡œì»¬ ëŒ€ì²´)"""
        
        # API ëª¨ë“œ ì‹œë„
        if use_api and openai_whisper_client.is_available():
            print("ğŸŒ OpenAI API ëª¨ë“œë¡œ ì „ì‚¬ ì‹œë„...")
            api_result = await openai_whisper_client.transcribe_audio_api(audio_path, language)
            
            if api_result.get("success"):
                print("âœ… API ì „ì‚¬ ì„±ê³µ!")
                return api_result
            else:
                print(f"âš ï¸ API ì „ì‚¬ ì‹¤íŒ¨, ë¡œì»¬ ëª¨ë“œë¡œ ëŒ€ì²´: {api_result.get('error')}")
        
        # ë¡œì»¬ ëª¨ë“œë¡œ ëŒ€ì²´
        print("ğŸ  ë¡œì»¬ ëª¨ë“œë¡œ ì „ì‚¬...")
        local_result = self.transcribe_audio_local(audio_path, model_name, language, task)
        
        if not local_result.get("success"):
            print("âŒ ë¡œì»¬ ì „ì‚¬ë„ ì‹¤íŒ¨!")
            return local_result
        
        # GPT í›„ì²˜ë¦¬ ì ìš© (ì˜µì…˜)
        if use_gpt_correction and gpt_postprocessor and gpt_postprocessor.is_available():
            print("ğŸ¤– GPT í›„ì²˜ë¦¬ë¡œ ì˜¤íƒ€ êµì • ì¤‘...")
            correction_result = await gpt_postprocessor.correct_segments(
                local_result.get("segments", []),
                context=f"í•œêµ­ì–´ ìŒì„± ì¸ì‹ ê²°ê³¼ êµì •"
            )
            
            if correction_result.get("success"):
                # êµì •ëœ ì„¸ê·¸ë¨¼íŠ¸ë¡œ ì—…ë°ì´íŠ¸
                corrected_segments = correction_result.get("corrected_segments", [])
                corrected_text = " ".join([seg.get("text", "") for seg in corrected_segments])
                
                local_result.update({
                    "text": corrected_text,
                    "segments": corrected_segments,
                    "gpt_correction_applied": correction_result.get("correction_applied", False),
                    "total_corrections": correction_result.get("total_corrections", 0),
                    "processing_method": f"{local_result.get('processing_method', 'unknown')} + GPTêµì •"
                })
                
                print(f"âœ… GPT êµì • ì™„ë£Œ: {correction_result.get('total_corrections', 0)}ê°œ ìˆ˜ì •")
            else:
                print(f"âš ï¸ GPT êµì • ì‹¤íŒ¨: {correction_result.get('error', 'Unknown error')}")
                # ì‹¤íŒ¨í•´ë„ ì›ë³¸ ê²°ê³¼ëŠ” ìœ ì§€
        elif use_gpt_correction:
            print("âš ï¸ GPT í›„ì²˜ë¦¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (API í‚¤ í™•ì¸ í•„ìš”)")
        
        return local_result
    
    def generate_srt(self, result: Dict) -> str:
        """ê²°ê³¼ë¥¼ SRT í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        srt_content = ""
        segments = result.get("segments", [])
        
        for i, segment in enumerate(segments, 1):
            start_time = self.seconds_to_srt_time(segment["start"])
            end_time = self.seconds_to_srt_time(segment["end"])
            text = segment["text"].strip()
            
            srt_content += f"{i}\n"
            srt_content += f"{start_time} --> {end_time}\n"
            srt_content += f"{text}\n\n"
        
        return srt_content
    
    def seconds_to_srt_time(self, seconds: float) -> str:
        """ì´ˆë¥¼ SRT ì‹œê°„ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        millisecs = int((seconds % 1) * 1000)
        
        return f"{hours:02d}:{minutes:02d}:{secs:02d},{millisecs:03d}"
    
    def get_audio_duration(self, audio_path: str) -> float:
        """ì˜¤ë””ì˜¤ íŒŒì¼ì˜ ê¸¸ì´ë¥¼ êµ¬í•¨"""
        try:
            probe = ffmpeg.probe(audio_path)
            duration = float(probe['streams'][0]['duration'])
            return duration
        except:
            return 60.0  # ê¸°ë³¸ê°’
    
    def create_video_with_subtitles(
        self, 
        audio_path: str, 
        srt_content: str, 
        output_path: str,
        background_color: str = "black"
    ):
        """ì˜¤ë””ì˜¤ì™€ ìë§‰ìœ¼ë¡œ ë¹„ë””ì˜¤ ìƒì„± (í•œêµ­ì–´ ìµœì í™”)"""
        try:
            # ì˜¤ë””ì˜¤ ê¸¸ì´ êµ¬í•˜ê¸°
            duration = self.get_audio_duration(audio_path)
            
            with tempfile.NamedTemporaryFile(mode='w', suffix='.srt', delete=False, encoding='utf-8') as srt_file:
                srt_file.write(srt_content)
                srt_path = srt_file.name
            
            # í•œêµ­ì–´ í°íŠ¸ ì„¤ì •
            font_style = (
                'FontSize=28,'
                'PrimaryColour=&Hffffff,'
                'OutlineColour=&H000000,'
                'Outline=3,'
                'Shadow=1,'
                'Alignment=2,'
                'MarginV=50'
            )
            
            # FFmpegë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„ë””ì˜¤ ìƒì„±
            cmd = [
                'ffmpeg',
                '-f', 'lavfi',
                '-i', f'color=c={background_color}:s=1280x720:d={duration}',
                '-i', audio_path,
                '-vf', f'subtitles={srt_path}:force_style=\'{font_style}\'',
                '-c:v', 'libx264',
                '-preset', 'medium',
                '-crf', '23',
                '-c:a', 'aac',
                '-b:a', '128k',
                '-shortest',
                '-y',
                output_path
            ]
            
            print(f"ğŸ¬ í•œêµ­ì–´ ìë§‰ ë¹„ë””ì˜¤ ìƒì„± ì¤‘: {output_path}")
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            if result.returncode != 0:
                raise Exception(f"FFmpeg ì˜¤ë¥˜: {result.stderr}")
            
            # ì„ì‹œ SRT íŒŒì¼ ì‚­ì œ
            os.unlink(srt_path)
            
            print(f"âœ… í•œêµ­ì–´ ìë§‰ ë¹„ë””ì˜¤ ìƒì„± ì™„ë£Œ: {output_path}")
            
        except Exception as e:
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            if 'srt_path' in locals() and os.path.exists(srt_path):
                os.unlink(srt_path)
            raise Exception(f"ë¹„ë””ì˜¤ ìƒì„± ì‹¤íŒ¨: {str(e)}")
    
    async def process_audio_to_video(
        self,
        audio_path: str,
        output_path: str,
        model_name: str = "large-v3",
        language: Optional[str] = "ko",
        task: str = "transcribe",
        background_color: str = "black",
        use_api: bool = False,
        use_gpt_correction: bool = False
    ) -> Dict:
        """ì „ì²´ í”„ë¡œì„¸ìŠ¤: ì˜¤ë””ì˜¤ â†’ ìë§‰ â†’ ë¹„ë””ì˜¤ (í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ)"""
        try:
            gpt_status = " + GPTêµì •" if use_gpt_correction else ""
            method = f"API + ë¡œì»¬ í•˜ì´ë¸Œë¦¬ë“œ{gpt_status}" if use_api else f"ë¡œì»¬{gpt_status}"
            print(f"ğŸš€ {method} ëª¨ë“œë¡œ ìŒì„± ì²˜ë¦¬ ì‹œì‘ - ëª¨ë¸: {model_name}")
            
            # 1. í•˜ì´ë¸Œë¦¬ë“œ ì˜¤ë””ì˜¤ ì „ì‚¬ + GPT í›„ì²˜ë¦¬
            print("ğŸ“ 1ë‹¨ê³„: í•˜ì´ë¸Œë¦¬ë“œ ìŒì„± ì¸ì‹ ì¤‘...")
            result = await self.transcribe_audio_hybrid(
                audio_path, model_name, language, task, use_api, use_gpt_correction
            )
            
            if not result.get("success"):
                return result
            
            # 2. SRT ìƒì„±
            print("ğŸ“„ 2ë‹¨ê³„: ìë§‰ ìƒì„± ì¤‘...")
            srt_content = self.generate_srt(result)
            
            # 3. ë¹„ë””ì˜¤ ìƒì„±
            print("ğŸ¬ 3ë‹¨ê³„: ìë§‰ ë¹„ë””ì˜¤ ìƒì„± ì¤‘...")
            self.create_video_with_subtitles(
                audio_path, srt_content, output_path, background_color
            )
            
            return {
                "success": True,
                "output_path": output_path,
                "transcript": result["text"],
                "segments_count": len(result.get("segments", [])),
                "language": result.get("language", "ko"),
                "language_probability": result.get("language_probability", 0.0),
                "model_used": model_name,
                "processing_method": result.get("processing_method", "unknown"),
                "gpt_correction_applied": result.get("gpt_correction_applied", False),
                "total_corrections": result.get("total_corrections", 0)
            }
        
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return {
                "success": False,
                "error": str(e)
            }

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
auto_subtitle = AutoSubtitle()
